{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQB-PO1fiNvZ"
      },
      "source": [
        "\n",
        "\n",
        "1.   Syed Muhammad Oaun\n",
        "2.   i212702 (BS-AI-J)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "N01MgrjR_RCR"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "976iqezUAnyf"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class LogisticRegression:\n",
        "    def __init__(self, learning_rate=0.01, max_iterations=1000, tolerance=1e-4, print_interval=10):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.max_iterations = max_iterations\n",
        "        self.tolerance = tolerance\n",
        "        self.print_interval = print_interval\n",
        "        self.theta = None\n",
        "\n",
        "    def hypothesis(self, X):\n",
        "        X = np.hstack((X, np.ones((X.shape[0], 1))))\n",
        "        #print(\"h\",X.shape)\n",
        "        return np.dot(X, self.theta.reshape(-1, 1))\n",
        "\n",
        "\n",
        "    def sigmoid(self,h):\n",
        "      return 1/(1 + np.exp(-h))\n",
        "\n",
        "    def der_cost_function(self, X, y):\n",
        "        h = self.hypothesis(X)\n",
        "        z = self.sigmoid(h)\n",
        "        y = y.reshape(-1, 1)\n",
        "        #print(h.shape)\n",
        "        #print(z.shape)\n",
        "        #print(y.shape)\n",
        "        #print((z-h).shape)\n",
        "        #print(X.shape)\n",
        "        X = np.hstack((X, np.ones((X.shape[0], 1))))\n",
        "        #print(X.shape)\n",
        "        return np.mean((z - y) * X, axis=0)\n",
        "\n",
        "    def cost_function(self, X, y):\n",
        "        h = self.hypothesis(X)\n",
        "        z = self.sigmoid(h)\n",
        "        y = y.reshape(-1, 1)\n",
        "        #print(z.shape)\n",
        "        #print(y.shape)\n",
        "\n",
        "        return np.mean(-(y * np.log(z) + (1 - y)* np.log(1 - z)))\n",
        "\n",
        "    def gradient_descent(self, X, y):\n",
        "        der = self.der_cost_function(X, y)\n",
        "        #print(\"der\",der.shape)\n",
        "        self.theta = self.theta - self.learning_rate * der\n",
        "\n",
        "    def get_weights(self):\n",
        "        return self.theta\n",
        "\n",
        "    def fit(self, X, y):\n",
        "\n",
        "        num_features = X.shape[1]\n",
        "        self.theta = np.zeros(num_features + 1)\n",
        "\n",
        "        for iteration in range(1, self.max_iterations + 1):\n",
        "            prev_theta = np.copy(self.theta)\n",
        "            self.gradient_descent(X, y)\n",
        "\n",
        "            if iteration % self.print_interval == 0:\n",
        "                loss = self.cost_function(X, y)\n",
        "                print(f\"Iteration {iteration}, Loss: {loss}\")\n",
        "                print(f\"weights \", self.theta)\n",
        "\n",
        "            if np.linalg.norm(self.theta - prev_theta) < self.tolerance:\n",
        "                print(f\"Converged at iteration {iteration}\")\n",
        "                break\n",
        "\n",
        "    def predict(self, X):\n",
        "        h = self.hypothesis(X)\n",
        "        z = self.sigmoid(h).flatten()\n",
        "        return z\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Features shape: (178, 13)\n",
            "Target shape: (178,)\n",
            "Classes: ['class_0' 'class_1' 'class_2']\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import load_wine\n",
        "\n",
        "# Load the dataset\n",
        "wine_data = load_wine()\n",
        "\n",
        "# Display some information about the dataset\n",
        "print(\"Features shape:\", wine_data.data.shape)\n",
        "print(\"Target shape:\", wine_data.target.shape)\n",
        "print(\"Classes:\", wine_data.target_names)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ORlwOTmQLmOL",
        "outputId": "5fcd7c97-7db0-4673-bf9c-543e5dfd340e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Num_classes 3 \n",
            "\n",
            "Original Y\n",
            " [1 0 0 1 1 0 2 2 1 1 0 1 0 2 0 2 1 0 2 0 1 0 0 0 1 0 2 2 0 0 1 1 2 0 1 1 0\n",
            " 1 1 1 1 0 0 2 2 1 1 1 0 2 0 2 2 1 2 0 0 2 2 1 1 1 1 0 1 2 1 0 0 0 2 0 2 1\n",
            " 1 2 1 0 1 0 1 0 2 1 0 2 1 2 2 1 0 0 0 1 1 1 2 1 1 2 2 1 2 2 2 1 1 2 1 2 2\n",
            " 2 1 1 2 0 1 1 0 0 0 0 1 1 0 2 1 2 1 0 1 1 1 1 0 1 2 0 0 2 0 2 1 1 1 2 0 0\n",
            " 0 2 0 1 1 1 2 1 0 2 1 1 0 2 2 0 1 0 0 0 2 1 2 0 0 1 0 1 0 1] \n",
            "\n",
            "binary_y_for class 0:\n",
            "[0 1 1 0 0 1 0 0 0 0 1 0 1 0 1 0 0 1 0 1 0 1 1 1 0 1 0 0 1 1 0 0 0 1 0 0 1\n",
            " 0 0 0 0 1 1 0 0 0 0 0 1 0 1 0 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 1 1 1 0 1 0 0\n",
            " 0 0 0 1 0 1 0 1 0 0 1 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 1 0 0 1 1 1 1 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 1 1 0 1 0 0 0 0 0 1 1\n",
            " 1 0 1 0 0 0 0 0 1 0 0 0 1 0 0 1 0 1 1 1 0 0 0 1 1 0 1 0 1 0]\n",
            "\n",
            "binary_y_for class 1:\n",
            "[1 0 0 1 1 0 0 0 1 1 0 1 0 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 1 1 0 0 1 1 0\n",
            " 1 1 1 1 0 0 0 0 1 1 1 0 0 0 0 0 1 0 0 0 0 0 1 1 1 1 0 1 0 1 0 0 0 0 0 0 1\n",
            " 1 0 1 0 1 0 1 0 0 1 0 0 1 0 0 1 0 0 0 1 1 1 0 1 1 0 0 1 0 0 0 1 1 0 1 0 0\n",
            " 0 1 1 0 0 1 1 0 0 0 0 1 1 0 0 1 0 1 0 1 1 1 1 0 1 0 0 0 0 0 0 1 1 1 0 0 0\n",
            " 0 0 0 1 1 1 0 1 0 0 1 1 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 1 0 1]\n",
            "\n",
            "binary_y_for class 2:\n",
            "[0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0\n",
            " 0 0 0 0 0 0 1 1 0 0 0 0 1 0 1 1 0 1 0 0 1 1 0 0 0 0 0 0 1 0 0 0 0 1 0 1 0\n",
            " 0 1 0 0 0 0 0 0 1 0 0 1 0 1 1 0 0 0 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 1 0 1 1\n",
            " 1 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 0 1 0 0\n",
            " 0 1 0 0 0 0 1 0 0 1 0 0 0 1 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.utils import shuffle\n",
        "#iris = datasets.load_iris()\n",
        "X = wine_data.data[:, :]\n",
        "y = wine_data.target\n",
        "X,y = shuffle(X,y)\n",
        "\n",
        "num_classes = len(np.unique(y))\n",
        "\n",
        "print(\"Num_classes\",num_classes,\"\\n\")\n",
        "\n",
        "print(\"Original Y\\n\", y,\"\\n\")\n",
        "\n",
        "binary_y=[]\n",
        "\n",
        "for i in range(num_classes):\n",
        "\n",
        "    binary_y.append((y == i).astype(int))      # class 0 is 1 every other 0\n",
        "    print(\"binary_y_for class {}:\\n{}\\n\".format(i, binary_y[i]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-p1fEUoS2l2"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "7pJEsCoQBG03"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test =[],[],[],[]\n",
        "X_train_scaled=[]\n",
        "\n",
        "for i in range(num_classes):\n",
        "\n",
        "    X_tr, X_te, y_tr, y_te = train_test_split(X, binary_y[i], test_size=0.20, random_state=42)\n",
        "    X_train.append(X_tr)\n",
        "    X_test.append(X_te)\n",
        "    y_train.append(y_tr)\n",
        "    y_test.append(y_te)\n",
        "\n",
        "\n",
        "    # Standardize the features\n",
        "    scaler = StandardScaler()\n",
        "    X_train_sc = scaler.fit_transform(X_tr)\n",
        "    X_train_scaled.append(X_train_sc)\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5GjLhIr6TU3W",
        "outputId": "decf3fd3-9d6b-41ca-d55f-887168b67a83"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "MODEL 0 TRAINING\n",
            "\n",
            "\n",
            "\n",
            "Iteration 100, Loss: 0.3619788602736398\n",
            "weights  [ 0.21878155 -0.04046472  0.08561773 -0.18044099  0.08560596  0.17137889\n",
            "  0.18860297 -0.10688333  0.07925723  0.04746026  0.07504653  0.16247216\n",
            "  0.27251545 -0.16180047]\n",
            "Iteration 200, Loss: 0.26275551720596013\n",
            "weights  [ 0.3550642  -0.0404788   0.14922426 -0.27760254  0.11957913  0.24152149\n",
            "  0.27443898 -0.14613758  0.09209642  0.07994969  0.10060166  0.24097899\n",
            "  0.43557572 -0.28879698]\n",
            "Iteration 300, Loss: 0.21221642379931954\n",
            "weights  [ 0.45526206 -0.03051907  0.19917061 -0.34548419  0.13508251  0.27911829\n",
            "  0.32732983 -0.16606914  0.08722663  0.10139592  0.11314788  0.29336082\n",
            "  0.55396765 -0.39259181]\n",
            "Iteration 400, Loss: 0.1803939543078145\n",
            "weights  [ 0.53477137 -0.01722038  0.24009903 -0.39878231  0.14228589  0.30231095\n",
            "  0.36535333 -0.17862908  0.07651858  0.11556995  0.12079022  0.33409779\n",
            "  0.64776633 -0.48033971]\n",
            "Iteration 500, Loss: 0.15807762081552698\n",
            "weights  [ 0.60073077 -0.0027276   0.27473757 -0.44329541  0.14533047  0.31787658\n",
            "  0.39521758 -0.18783185  0.06393173  0.12491832  0.12612429  0.36835379\n",
            "  0.72582382 -0.55631953]\n",
            "Iteration 600, Loss: 0.14137414692116718\n",
            "weights  [ 0.6571065   0.01208705  0.30479981 -0.48186922  0.14618267  0.32893494\n",
            "  0.42002029 -0.19529713  0.05101447  0.13099753  0.13019383  0.39844431\n",
            "  0.7928803  -0.62327436]\n",
            "Iteration 700, Loss: 0.12831092721524298\n",
            "weights  [ 0.70634905  0.02682404  0.33141578 -0.51610936  0.14586217  0.3371264\n",
            "  0.44140678 -0.20176027  0.03841507  0.13481845  0.1334843   0.42558181\n",
            "  0.85178743 -0.68307378]\n",
            "Iteration 800, Loss: 0.11776527451475946\n",
            "weights  [ 0.75008246  0.04129054  0.35535923 -0.54701559  0.14492748  0.3433965\n",
            "  0.46034068 -0.20758564  0.02640299  0.13705399  0.13624681  0.4504776\n",
            "  0.90440031 -0.73705427]\n",
            "Iteration 900, Loss: 0.10904426280439276\n",
            "weights  [ 0.78943861  0.05539384  0.37717672 -0.57525782  0.14369525  0.34832799\n",
            "  0.47742882 -0.21296729  0.01507604  0.13816228  0.13862307  0.47358691\n",
            "  0.95199615 -0.78620747]\n",
            "Iteration 1000, Loss: 0.10169381422458731\n",
            "weights  [ 0.82523727  0.06909291  0.39726488 -0.6013108   0.14234844  0.35229861\n",
            "  0.49307534 -0.21801555  0.00445108  0.13846137  0.14069949  0.49522147\n",
            "  0.99549317 -0.83129124]\n",
            "Model Weights: [ 0.82523727  0.06909291  0.39726488 -0.6013108   0.14234844  0.35229861\n",
            "  0.49307534 -0.21801555  0.00445108  0.13846137  0.14069949  0.49522147\n",
            "  0.99549317 -0.83129124]\n",
            "\n",
            "\n",
            "MODEL 1 TRAINING\n",
            "\n",
            "\n",
            "\n",
            "Iteration 100, Loss: 0.4101993464887916\n",
            "weights  [-0.2767457  -0.12059234 -0.13446239  0.08091058 -0.09737898 -0.01396188\n",
            "  0.02718133  0.00521002  0.03867568 -0.2538562   0.14119955  0.06643829\n",
            " -0.23108862 -0.08132253]\n",
            "Iteration 200, Loss: 0.30078827186858204\n",
            "weights  [-0.44971331 -0.19773444 -0.21725869  0.13669596 -0.147038   -0.01998146\n",
            "  0.04682765  0.01578107  0.06853813 -0.40621324  0.22771122  0.10068027\n",
            " -0.3807253  -0.14674769]\n",
            "Iteration 300, Loss: 0.24393311371615256\n",
            "weights  [-0.57347724 -0.25475196 -0.2770173   0.17854867 -0.17655225 -0.02227742\n",
            "  0.06300236  0.02752196  0.09268543 -0.51280566  0.29026284  0.12134876\n",
            " -0.49068986 -0.20178245]\n",
            "Iteration 400, Loss: 0.2087347161074313\n",
            "weights  [-0.66966654 -0.30043195 -0.32445987  0.21189982 -0.19610236 -0.02280198\n",
            "  0.07693472  0.03928518  0.11271036 -0.59479184  0.33974657  0.13495889\n",
            " -0.57813467 -0.24958244]\n",
            "Iteration 500, Loss: 0.1845195614289931\n",
            "weights  [-0.74835647 -0.33879101 -0.36431807  0.23957046 -0.21003753 -0.02240144\n",
            "  0.08923893  0.05071375  0.12963348 -0.66168875  0.3809951   0.14434693\n",
            " -0.65116202 -0.2920023 ]\n",
            "Iteration 600, Loss: 0.16667261243874326\n",
            "weights  [-0.81497287 -0.37198727 -0.39902689  0.26318614 -0.22050195 -0.02147334\n",
            "  0.1003007   0.06169208  0.14415165 -0.71847353  0.41655943  0.15097994\n",
            " -0.71416397 -0.3302285 ]\n",
            "Iteration 700, Loss: 0.1528715125465372\n",
            "weights  [-0.87275767 -0.40131846 -0.42998949  0.28376482 -0.22867127 -0.02022124\n",
            "  0.1103846   0.07219192  0.15675963 -0.768044    0.44795131  0.15570814\n",
            " -0.76977611 -0.36507399]\n",
            "Iteration 800, Loss: 0.14181576575223487\n",
            "weights  [-0.9237997  -0.42763013 -0.45808528  0.30198558 -0.23524312 -0.01875757\n",
            "  0.11968049  0.08221992  0.16781973 -0.81222385  0.47614198  0.15906415\n",
            " -0.81970286 -0.39712502]\n",
            "Iteration 900, Loss: 0.13271739968544569\n",
            "weights  [-0.96952338 -0.45150703 -0.48390202  0.31832386 -0.24065736 -0.01714855\n",
            "  0.12832877  0.0917971   0.17760431 -0.85222992  0.5017934   0.16140082\n",
            " -0.86511066 -0.42682107]\n",
            "Iteration 1000, Loss: 0.12506931346548805\n",
            "weights  [-1.01094361 -0.47337238 -0.50785287  0.33312545 -0.24520495 -0.01543549\n",
            "  0.13643575  0.10094993  0.18632268 -0.88891193  0.52537764  0.16296199\n",
            " -0.90683363 -0.45450168]\n",
            "Model Weights: [-1.01094361 -0.47337238 -0.50785287  0.33312545 -0.24520495 -0.01543549\n",
            "  0.13643575  0.10094993  0.18632268 -0.88891193  0.52537764  0.16296199\n",
            " -0.90683363 -0.45450168]\n",
            "\n",
            "\n",
            "MODEL 2 TRAINING\n",
            "\n",
            "\n",
            "\n",
            "Iteration 100, Loss: 0.35134071698060254\n",
            "weights  [ 0.06134063  0.1626444   0.05024085  0.09873453  0.01256569 -0.15703135\n",
            " -0.2157959   0.10220016 -0.11787456  0.20998351 -0.21734665 -0.22964696\n",
            " -0.03884619 -0.19912715]\n",
            "Iteration 200, Loss: 0.2520690128998626\n",
            "weights  [ 0.11031909  0.24558226  0.07404261  0.13709624  0.03048914 -0.21990936\n",
            " -0.32111557  0.13226768 -0.16097273  0.34217524 -0.33237243 -0.34412864\n",
            " -0.04307036 -0.35569703]\n",
            "Iteration 300, Loss: 0.2022082824605327\n",
            "weights  [ 0.14892504  0.2999538   0.0887867   0.15919172  0.04631243 -0.25399848\n",
            " -0.38978232  0.1414097  -0.18150921  0.44125117 -0.40972095 -0.41810549\n",
            " -0.04065268 -0.48422131]\n",
            "Iteration 400, Loss: 0.17111370355594588\n",
            "weights  [ 0.18011367  0.33979788  0.09945855  0.17504834  0.05969032 -0.27582973\n",
            " -0.44120386  0.14247383 -0.19270747  0.52130197 -0.46771306 -0.47235203\n",
            " -0.03700198 -0.59344987]\n",
            "Iteration 500, Loss: 0.14949219954759702\n",
            "weights  [ 0.20594845  0.37086492  0.10802051  0.18791579  0.07107347 -0.2913372\n",
            " -0.48276999  0.13997694 -0.19930317  0.58874408 -0.51389447 -0.51513104\n",
            " -0.03344303 -0.68855787]\n",
            "Iteration 600, Loss: 0.1334354026949872\n",
            "weights  [ 0.22782231  0.39609376  0.1153884   0.19911086  0.08090996 -0.30315811\n",
            " -0.51799513  0.13583204 -0.2033839   0.64713731 -0.55212519 -0.55049061\n",
            " -0.03029952 -0.77284589]\n",
            "Iteration 700, Loss: 0.1209681485589708\n",
            "weights  [ 0.24669327  0.41719211  0.12203388  0.20925207  0.08954703 -0.31263575\n",
            " -0.54880133  0.13093976 -0.20599607  0.69869303 -0.58465326 -0.58068796\n",
            " -0.02760999 -0.84855617]\n",
            "Iteration 800, Loss: 0.11096898353702703\n",
            "weights  [ 0.26323716  0.4352384   0.12821638  0.21866333  0.09724139 -0.32052343\n",
            " -0.57634627  0.12575199 -0.20771176  0.74488956 -0.61290465 -0.60709899\n",
            " -0.02533128 -0.91728742]\n",
            "Iteration 900, Loss: 0.10274782517550554\n",
            "weights  [ 0.27794257  0.45095465  0.13408558  0.22752963  0.10418217 -0.3272755\n",
            " -0.60137856  0.12050375 -0.20886389  0.78676786 -0.63784024 -0.63061936\n",
            " -0.0234023  -0.98022295]\n",
            "Iteration 1000, Loss: 0.09585416086247783\n",
            "weights  [ 0.29117064  0.46484576  0.13973107  0.23596457  0.11051016 -0.33318223\n",
            " -0.62440944  0.11531887 -0.20965546  0.82509024 -0.66013739 -0.65186243\n",
            " -0.02176432 -1.03826507]\n",
            "Model Weights: [ 0.29117064  0.46484576  0.13973107  0.23596457  0.11051016 -0.33318223\n",
            " -0.62440944  0.11531887 -0.20965546  0.82509024 -0.66013739 -0.65186243\n",
            " -0.02176432 -1.03826507]\n"
          ]
        }
      ],
      "source": [
        "logistic_reg = []   # models to store here\n",
        "predictions = []\n",
        "for i in range(num_classes):\n",
        "\n",
        "    print(\"\\n\\nMODEL {} TRAINING\\n\\n\\n\".format(i))\n",
        "    \n",
        "    logistic_reg.append(LogisticRegression(learning_rate=0.01, max_iterations=1000, tolerance=1e-4, print_interval=100))\n",
        "    logistic_reg[i].fit(X_train_scaled[i], y_train[i])\n",
        "\n",
        "    weights = logistic_reg[i].get_weights()\n",
        "    print(\"Model Weights:\", weights)\n",
        "\n",
        "    X_test_scaled = scaler.transform(X_test[i])\n",
        "    predictions.append(logistic_reg[i].predict(X_test_scaled))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "IURsxosJBJBn"
      },
      "outputs": [],
      "source": [
        "def accuracy(predictions, y_true):\n",
        "\n",
        "    print(y_true)\n",
        "    correct_predictions = np.sum(predictions.round() == y_true)    # rounds to the most near value\n",
        "    print(predictions.round())\n",
        "    total_samples = len(y_true)\n",
        "    accuracy = correct_predictions / total_samples\n",
        "    return accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CtIr4BTAUVOa",
        "outputId": "b26b0e56-4751-4ec5-86cc-3e829882bc2c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1 0 0 0 0 0 0 0 1 1 0 1 1 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "[1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Model 0 \n",
            " 1.0\n",
            "\n",
            "[0 1 0 1 1 1 0 1 0 0 0 0 0 1 0 0 1 1 1 0 0 1 1 0 1 0 1 1 0 0 0 0 1 1 0 1]\n",
            "[0. 1. 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 1. 0.\n",
            " 1. 0. 1. 1. 0. 0. 0. 0. 1. 1. 0. 1.]\n",
            "Model 1 \n",
            " 1.0\n",
            "\n",
            "[0 0 1 0 0 0 1 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 1 1 1 0 0 1 0]\n",
            "[0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1.]\n",
            "Model 2 \n",
            " 0.9166666666666666\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for i in range(num_classes):\n",
        "\n",
        "    print(\"Model {} \\n {}\\n\".format(i,accuracy(predictions[i], y_test[i])))\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "uEL3IiZbWE7F"
      },
      "outputs": [],
      "source": [
        "def accuracy_(predictions, y_true):\n",
        "        # Assuming y_true is the actual labels for the test set\n",
        "        print(predictions)\n",
        "        mse = np.mean((predictions - y_true) ** 2)\n",
        "        accuracy = 1 - mse / np.var(y_true)\n",
        "        return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CxdO0jNCUny_",
        "outputId": "0db2c824-c6a7-4628-8841-e6d832c8e97a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.95389882 0.0696683  0.03923718 0.07275303 0.00984067 0.04890862\n",
            " 0.03222042 0.01710431 0.98892615 0.96361096 0.39739254 0.9829589\n",
            " 0.98541168 0.144484   0.03495142 0.02499854 0.00792474 0.02526591\n",
            " 0.02756559 0.94528319 0.98333279 0.09387218 0.05103453 0.99841752\n",
            " 0.01587984 0.051627   0.0412504  0.24177425 0.11811027 0.01641328\n",
            " 0.20855314 0.02878945 0.02712921 0.02256421 0.0255109  0.02850132]\n",
            "Model 0 \n",
            " 0.9452445201011306\n",
            "\n",
            "[0.09234832 0.75358929 0.16150713 0.72315136 0.9992295  0.97830104\n",
            " 0.10807816 0.97784318 0.0228685  0.14635978 0.00569691 0.07647684\n",
            " 0.0508793  0.94183609 0.03557515 0.49123171 0.95673168 0.98287641\n",
            " 0.93388224 0.0588036  0.02086827 0.84821853 0.76768266 0.00386653\n",
            " 0.9803924  0.03672269 0.63194775 0.9392996  0.01414824 0.06954739\n",
            " 0.00830039 0.20268599 0.6453013  0.85376671 0.19243651 0.67780494]\n",
            "Model 1 \n",
            " 0.8859202151712435\n",
            "\n",
            "[0.05429569 0.2618128  0.92783043 0.27822373 0.00383372 0.02771451\n",
            " 0.962829   0.07390997 0.06583086 0.02658677 0.97632798 0.02623163\n",
            " 0.03538364 0.02654385 0.98841104 0.78453069 0.25599166 0.04152139\n",
            " 0.14445415 0.10809851 0.10167484 0.11472497 0.31706531 0.06731892\n",
            " 0.07292432 0.98378149 0.51933768 0.01424937 0.98575663 0.98807475\n",
            " 0.98481723 0.92843662 0.64049767 0.35466258 0.94218595 0.5651586 ]\n",
            "Model 2 \n",
            " 0.7925252008762387\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for i in range(num_classes):\n",
        "\n",
        "    print(\"Model {} \\n {}\\n\".format(i,accuracy_(predictions[i], y_test[i])))\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Selecting MAX Confidence one"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "id": "5fJ38PcpgZfC"
      },
      "outputs": [],
      "source": [
        "def predict_1_vs_all(models,X_test):\n",
        "    \n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "    predictions = []\n",
        "\n",
        "    for i in range(len(models)):\n",
        "    \n",
        "        predictions.append(models[i].predict(X_test_scaled))\n",
        "\n",
        "\n",
        "    max_index=[]\n",
        "    for i in range(len(predictions[0])):\n",
        "        max_index.append(np.argmax([scores[i] for scores in predictions]))\n",
        "\n",
        "    return max_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s9l1o4sMgek0",
        "outputId": "ec331f1e-9e26-41f9-c131-082adb0e7288"
      },
      "outputs": [],
      "source": [
        "X_t, X_tes, y_t, y_tes = train_test_split(X, y, test_size=0.20, random_state=42)\n",
        "p=predict_1_vs_all(logistic_reg,X_tes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "id": "pL3tkVtIhCzN"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0, 1, 0, 1, 0, 1, 0, 0, 2, 2, 1, 0, 0, 0, 2, 2, 1, 2, 1, 2, 2, 1, 0, 1, 0, 1, 1, 2, 0, 0, 0, 1, 2, 2, 0, 0]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.9103362391033624"
            ]
          },
          "execution_count": 118,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "accuracy_(p, y_tes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 0,\n",
              " 0]"
            ]
          },
          "execution_count": 120,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "p"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
